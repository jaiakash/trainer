{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629c902e-6cd0-4475-b6ce-5d6e37d7e2f3",
   "metadata": {},
   "source": [
    "# T5 LLM Fine-Tuning with DeepSpeed and Kubeflow Trainer\n",
    "\n",
    "\n",
    "This Notebook will fine-tune Text-to-Text Transfer Transformer (T5) with Wikihow dataset for text summarization using Kubeflow TrainJob and DeepSpeed.\n",
    "\n",
    "Pretrained T5 model: https://huggingface.co/google-t5/t5-base\n",
    "\n",
    "Wikihow dataset: https://huggingface.co/datasets/sentence-transformers/wikihow\n",
    "\n",
    "This Notebook will use **4 x A100 NVIDIA GPUs**, to fine-tune T5 model on 2 nodes (every node has 2 GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c461b2984e77d99",
   "metadata": {},
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900404c5d532bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/kubeflow/sdk.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47534ee4955f3ff6",
   "metadata": {},
   "source": [
    "## Create Script to Fine-Tune T5 with DeepSpeed\n",
    "\n",
    "We need to wrap our fine-tuning script into a function to create Kubeflow TrainJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f06c45b614ecd0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def deepspeed_train_t5(args):\n",
    "    import os\n",
    "    import time\n",
    "    import boto3\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    from torch.utils.data.distributed import DistributedSampler\n",
    "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "    from datasets import load_dataset\n",
    "    import deepspeed\n",
    "    import numpy as np\n",
    "\n",
    "    # Initialize distributed environment.\n",
    "    deepspeed.init_distributed(dist_backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "    # Define the Wikihow dataset class\n",
    "    class wikihow(torch.utils.data.Dataset):\n",
    "        def __init__(self, tokenizer, num_samples):\n",
    "            self.dataset = load_dataset(\n",
    "                \"sentence-transformers/wikihow\", split=f\"train[:{num_samples}]\"\n",
    "            )\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def clean_text(self, text):\n",
    "            if text is None:\n",
    "                return \"\"\n",
    "\n",
    "            return text.replace(\"\\n\", \" \").replace(\"``\", \"\").replace('\"', \"\").strip()\n",
    "\n",
    "        def convert_to_features(self, example_batch):\n",
    "            input_ = self.clean_text(example_batch[\"text\"])\n",
    "            target_ = self.clean_text(example_batch[\"summary\"])\n",
    "\n",
    "            source = self.tokenizer(\n",
    "                input_,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_,\n",
    "                max_length=150,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            return source, targets\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            source, targets = self.convert_to_features(self.dataset[index])\n",
    "            return {\n",
    "                \"source_ids\": source[\"input_ids\"].squeeze(),\n",
    "                \"source_mask\": source[\"attention_mask\"].squeeze(),\n",
    "                \"target_ids\": targets[\"input_ids\"].squeeze(),\n",
    "                \"target_mask\": targets[\"attention_mask\"].squeeze(),\n",
    "            }\n",
    "\n",
    "    # Download model and tokenizer.\n",
    "    if dist.get_rank() == 0:\n",
    "        print(\"-\" * 100)\n",
    "        print(\"Downloading T5 Model\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(args[\"MODEL_NAME\"])\n",
    "    tokenizer = T5Tokenizer.from_pretrained(args[\"MODEL_NAME\"])\n",
    "\n",
    "    # Download dataset.\n",
    "    dataset = wikihow(tokenizer, num_samples=int(args[\"NUM_SAMPLES\"]))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=4, sampler=DistributedSampler(dataset)\n",
    "    )\n",
    "\n",
    "    # Define DeepSpeed configuration.\n",
    "    # Train batch size = micro batch size * gradient steps * GPUs (e.g. 2 x 1 x 8 = 16).\n",
    "    ds_config = {\n",
    "        \"train_micro_batch_size_per_gpu\": 2,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        # \"fp16\": {\"enabled\": True}, # If your GPU (e.g. V100) doesn't support bf16, use fp16.\n",
    "        \"bf16\": {\"enabled\": True},  # Enable mixed precision.\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\"lr\": 0.002},\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"WarmupLR\",\n",
    "            \"params\": {\n",
    "                \"warmup_min_lr\": 0,\n",
    "                \"warmup_max_lr\": 0.001,\n",
    "                \"warmup_num_steps\": 1000,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Initialize model with DeepSpeed.\n",
    "    model, _, _, _ = deepspeed.initialize(\n",
    "        config=ds_config,\n",
    "        model=model,\n",
    "        model_parameters=model.parameters(),\n",
    "    )\n",
    "\n",
    "    # Start training process.\n",
    "    if dist.get_rank() == 0:\n",
    "        print(\"-\" * 100)\n",
    "        print(\"Starting DeepSpeed distributed training...\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for epoch in range(1, 3):\n",
    "        losses = []\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            for key in batch.keys():\n",
    "                batch[key] = batch[key].to(local_rank)\n",
    "            # Forward pass.\n",
    "            output = model(\n",
    "                input_ids=batch[\"source_ids\"],\n",
    "                attention_mask=batch[\"source_mask\"],\n",
    "                labels=batch[\"target_ids\"],\n",
    "            )\n",
    "            loss = output.loss\n",
    "\n",
    "            # Run backpropagation.\n",
    "            model.backward(loss)\n",
    "            # Weight updates.\n",
    "            model.step()\n",
    "            losses.append(loss.item())\n",
    "            if batch_idx % 10 == 0 and dist.get_rank() == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(batch),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if dist.get_rank() == 0:\n",
    "            print(\"-\" * 100)\n",
    "            print(\"Average Train Loss: {0:.4f}\".format(np.mean(losses)))\n",
    "            print(\"-\" * 100)\n",
    "\n",
    "    if dist.get_rank() == 0:\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"DeepSpeed training time: {int(time.time() - t0)} seconds\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        print(\"Exporting HuggingFace model to S3\")\n",
    "        MODEL_PATH = os.path.join(\"/home/mpiuser\", args[\"MODEL_NAME\"])\n",
    "        model.module.save_pretrained(MODEL_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n",
    "        bucket = boto3.resource(\"s3\").Bucket(args[\"BUCKET\"])\n",
    "        for file in os.listdir(MODEL_PATH):\n",
    "            print(f\"Uploading file {os.path.join(MODEL_PATH, file)}\")\n",
    "            bucket.upload_file(\n",
    "                os.path.join(MODEL_PATH, file), os.path.join(args[\"MODEL_NAME\"], file)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875abc44-69dd-41d8-bb2a-436f730a2dd0",
   "metadata": {},
   "source": [
    "## List Available Kubeflow Trainer Runtimes\n",
    "\n",
    "\n",
    "Get available Kubeflow Trainer Runtimes with the `list_runtimes()` API.\n",
    "\n",
    "You can inspect Runtime details, including the name, framework, and available devices on the single node.\n",
    "\n",
    "- Runtimes with **CustomTrainer**: You must write the training script within the function.\n",
    "\n",
    "- Runtimes with **BuiltinTrainer**: You can configure settings (e.g., LoRA Config) for LLM fine-tuning Job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8bc1d-8d9b-48f7-866f-c6ad4ad4241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import TrainerClient, CustomTrainer\n",
    "\n",
    "for r in TrainerClient().list_runtimes():\n",
    "    if r.name == \"deepspeed-distributed\":\n",
    "        print(f\"Name: {r.name}, Framework: {r.trainer.framework}, Trainer Type: {r.trainer.trainer_type.value}\\n\")\n",
    "        print(f\"Runtime devices: {r.trainer.device} x {r.trainer.device_count}\")\n",
    "        deepspeed_runtime = r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c389d-9dcb-474f-b0f5-cc098de2e183",
   "metadata": {},
   "source": [
    "## Create TrainJob for Distributed Training\n",
    "\n",
    "Use the `train()` API to scale the training code across 2 Nodes and 8 GPUs.\n",
    "\n",
    "Don't forget to update **the S3 bucket** name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe265b-9e07-4f5a-b4fb-5dfac8a61a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "# BUCKET_NAME = \"TODO: add your bucket here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dab189-f184-4e48-be74-f32c0dea675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"NUM_SAMPLES\": \"2000\",\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"BUCKET\": BUCKET_NAME,\n",
    "}\n",
    "\n",
    "job_id = TrainerClient().train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=deepspeed_train_t5,\n",
    "        func_args=args,\n",
    "        packages_to_install=[\"boto3\"], # Custom packages to install at runtime.\n",
    "        num_nodes=1,\n",
    "    ),\n",
    "    runtime=deepspeed_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1811dd-4bf2-40cf-ad35-35a87271eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train API generates a random TrainJob id.\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0864121-895f-4e5a-87b8-7ea2d92e6630",
   "metadata": {},
   "source": [
    "## Check the TrainJob Info\n",
    "\n",
    "Use the `list_jobs()` and `get_job()` APIs to get information about created TrainJob and its steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a945930-6cfe-4388-8ab8-462474f3f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in TrainerClient().list_jobs():\n",
    "    print(f\"TrainJob: {job.name}, Status: {job.status}, Created at: {job.creation_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8c741-6d31-49a2-8667-d38e40d62430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We execute mpirun command on node-0, which functions as the MPI Launcher node.\n",
    "for c in TrainerClient().get_job(name=job_id).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa3ea2-bdf5-40ca-a9ec-cc948949ea59",
   "metadata": {},
   "source": [
    "## Get the TrainJob Logs\n",
    "\n",
    "Use the `get_job_logs()` API to retrieve the TrainJob logs.\n",
    "\n",
    "Since we distribute the dataset accross 4 GPUs (2 nodes x 2 GPUs), each rank processes `round(2000 / 4) = 500` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e630fd3-f061-4fea-8024-7bffcefb257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = TrainerClient().get_job_logs(name=job_id, follow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ea83c-9b1c-477a-a3f3-72d659066678",
   "metadata": {},
   "source": [
    "## Download the Trained Model\n",
    "\n",
    "Finally, download fine-tuned model from S3 for evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0bfea-9fea-4616-bf43-d945d768e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DIR = \"./t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d5623-3425-41ea-87e7-ec6992540994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "s3 = boto3.client(\"s3\")\n",
    "for obj in s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=\"t5-base\")[\"Contents\"]:\n",
    "    file = obj[\"Key\"]\n",
    "\n",
    "    print(f\"Downloading file: {file}\")\n",
    "    s3.download_file(BUCKET_NAME, file, os.path.join(LOCAL_DIR, os.path.basename(file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075e5d9-62ec-495e-9bc8-18b5b3269e25",
   "metadata": {},
   "source": [
    "## Evaluate Fine-Tuned T5 Model\n",
    "\n",
    "After model is downloaded, you can load it into the HuggingFace pipeline.\n",
    "\n",
    "The T5 model performs well for NLP tasks such as summarization, translation, and text classification.\n",
    "\n",
    "In the example below, we'll demonstrate how to use a fine-tuned version of the T5 model to summarize documentation related to the Kubeflow Trainer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54caa0d-bd5a-4c3d-862b-3041072461b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the fine-tuned T5 model.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(LOCAL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_DIR)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"pt\")\n",
    "\n",
    "text = \"\"\"\n",
    "summarize: In Kubeflow Trainer you can integrate other ML libraries such as HuggingFace,\n",
    "DeepSpeed, or Megatron-LM with Kubeflow Trainer to orchestrate their ML training on Kubernetes.\n",
    "Kubeflow Trainer allows you to effortlessly develop your LLMs with the Kubeflow Python SDK\n",
    "and build Kubernetes-native Training Runtimes with Kubernetes Custom Resources APIs.\n",
    "Kubeflow Trainer is a Kubernetes-native project designed for large language models (LLMs)\n",
    "fine-tuning and enabling scalable, distributed training of machine learning (ML)\n",
    "models across various frameworks, including PyTorch, JAX, TensorFlow, and XGBoost.\n",
    "\"\"\"\n",
    "\n",
    "summarizer(text, min_length=5, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c68c68-9613-400c-9937-23b54d4d4b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
